{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ccb6b7",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "def read_file(file_path, file_type):\n",
    "    if file_type.lower() == 'csv':\n",
    "        df = pd.read_csv(file_path)\n",
    "    elif file_type.lower() == 'xlsx':\n",
    "        df = pd.read_excel(file_path)\n",
    "    elif file_type.lower() == 'url':\n",
    "        response = requests.get(file_path)\n",
    "        data = response.content.decode('utf-8')\n",
    "        df = pd.read_csv(StringIO(data))\n",
    "    elif file_type.lower() == 'json':\n",
    "        df = pd.read_json(file_path)\n",
    "    elif file_type.lower() == 'text':\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = file.read()\n",
    "        df = pd.DataFrame({'text': [data]})\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file type. Supported types are: CSV, XLSX, URL, JSON, Text\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "file_path = input(\"Enter the file path or URL: \")\n",
    "file_type = input(\"Enter the file type (CSV, XLSX, URL, JSON, Text): \")\n",
    "\n",
    "df = read_file(file_path, file_type)\n",
    "\n",
    "#C:\\Users\\LENOVO\\cnn_5550296508.csv\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1601a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb03763",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe8372",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9231faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "# Replace null values\n",
    "df['description'].fillna('unknown', inplace=True)\n",
    "df['name'].fillna('unknown', inplace=True)\n",
    "df['link'].fillna('no link', inplace=True)\n",
    "df['picture'].fillna('no link', inplace=True)\n",
    "df['caption'].fillna('no caption', inplace=True)\n",
    "df['status_type'].fillna('others', inplace=True)\n",
    "# Drop rows with null values in the \"message\" column\n",
    "df.dropna(subset=['message'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2606f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e37ecc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Get unique post types and their counts\n",
    "post_type_counts = df['post_type'].value_counts()\n",
    "\n",
    "# Generate random colors\n",
    "colors = np.random.rand(len(post_type_counts), 3)\n",
    "\n",
    "# Plot bar plot with random colors\n",
    "ax = post_type_counts.plot(kind='bar', color=colors)\n",
    "plt.title('Bar Plot of Post Types')\n",
    "plt.xlabel('Post Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add count labels on top of each bar\n",
    "for i, count in enumerate(post_type_counts):\n",
    "    ax.text(i, count, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3544894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df[['likes_count', 'comments_count', 'shares_count', 'love_count', 'wow_count', 'haha_count', 'sad_count', 'thankful_count', 'angry_count']].corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b9919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the occurrences of each status type\n",
    "status_type_counts = df['status_type'].value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = status_type_counts.plot(kind='bar', color=colors)\n",
    "plt.title('Distribution of Status Types')\n",
    "plt.xlabel('Status Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.grid(axis='y')\n",
    "\n",
    "# Add counts on top of each bar\n",
    "for bar in bars.patches:\n",
    "    plt.annotate(format(bar.get_height(), '.0f'),\n",
    "                 (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                 ha='center', va='center',\n",
    "                 xytext=(0, 5),\n",
    "                 textcoords='offset points')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87463b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d112bb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65b9fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8767fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8463a24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Initialize TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Initialize WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def pos_tagging_and_lemmatization(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Lowercase conversion\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Remove punctuation and special symbols\n",
    "    tokens = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    # Perform POS tagging\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Lemmatization with POS tagging\n",
    "    lemmatized_tokens = []\n",
    "    for word, tag in pos_tags:\n",
    "        pos = tag[0].lower()  # Convert POS tag to lowercase\n",
    "        pos = pos if pos in ['a', 'n', 'v'] else 'n'  # Map POS tag to WordNet POS tag\n",
    "        lemma = lemmatizer.lemmatize(word, pos=pos)  # Perform lemmatization\n",
    "        lemmatized_tokens.append(lemma)\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def preprocess_nlp_columns(df):\n",
    "    columns = input(\"Enter column names separated by spaces: \").split()\n",
    "    for column in columns:\n",
    "        df['tokenized_' + column + '_lemmatized_pos'] = df[column].apply(pos_tagging_and_lemmatization)\n",
    "        \n",
    "preprocess_nlp_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e871c6",
   "metadata": {},
   "source": [
    "This code snippet performs text preprocessing using NLTK and defines a function `pos_tagging_and_lemmatization` to tokenize, remove stopwords, remove punctuation, perform POS tagging, and lemmatize the text. Here's an explanation of each part:\n",
    "\n",
    "1. **NLTK Imports**:\n",
    "   - `nltk` is imported, which is a natural language processing library in Python.\n",
    "   - Various modules from NLTK are imported:\n",
    "     - `TweetTokenizer`: A tokenizer designed specifically for tweets.\n",
    "     - `word_tokenize`: A function to tokenize words from sentences.\n",
    "     - `stopwords`: A corpus of stopwords in different languages.\n",
    "     - `WordNetLemmatizer`: A lemmatizer based on WordNet.\n",
    "\n",
    "2. **Initializing Tokenizer and Lemmatizer**:\n",
    "   - `TweetTokenizer` and `WordNetLemmatizer` objects are initialized.\n",
    "\n",
    "3. **POS Tagging and Lemmatization Function** (`pos_tagging_and_lemmatization`):\n",
    "   - This function takes a text input and performs the following steps:\n",
    "     - Tokenizes the text using `word_tokenize`.\n",
    "     - Converts tokens to lowercase.\n",
    "     - Removes stopwords using the English stopwords provided by NLTK.\n",
    "     - Removes punctuation using the `string.punctuation` module.\n",
    "     - Performs POS tagging using `pos_tag`.\n",
    "     - Lemmatizes each token based on its POS tag using `lemmatizer.lemmatize`.\n",
    "     - Joins the lemmatized tokens back into a string and returns it.\n",
    "\n",
    "4. **Applying POS Tagging and Lemmatization**:\n",
    "   - The `pos_tagging_and_lemmatization` function is applied to the 'message' column of the DataFrame (`df['message']`) and the result is stored in a new column named 'tokenized_message_lemmatized_pos'.\n",
    "\n",
    "Overall, this code prepares text data for further analysis or modeling by tokenizing, removing stopwords and punctuation, performing POS tagging, and lemmatizing the text. This preprocessing step is commonly used in natural language processing tasks to improve the quality of text data for downstream tasks like sentiment analysis, classification, or information retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8558a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis and assign sentiment labels\n",
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(str(text))\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 'Positive'\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Negative'\n",
    "\n",
    "df['sentiment_label'] = df['tokenized_message_lemmatized_pos'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bddefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392db712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count for each message\n",
    "df['word_count'] = df['tokenized_message_lemmatized_pos'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate average word count\n",
    "average_word_count = df['word_count'].mean()\n",
    "print(\"Average Word Count:\", average_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712ff360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "def feature_extraction(df, text_column, numerical_features, target_column):\n",
    "    # Text Data Processing\n",
    "    tokenizer = Tokenizer(num_words=10000)\n",
    "    tokenizer.fit_on_texts(df[text_column])\n",
    "    X_text = tokenizer.texts_to_sequences(df[text_column])\n",
    "    X_text = pad_sequences(X_text, maxlen=100)\n",
    "\n",
    "    # Adding numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_numerical = df[numerical_features].values\n",
    "    X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "    # Concatenate text and numerical features\n",
    "    X = np.concatenate((X_text, X_numerical_scaled), axis=1)\n",
    "\n",
    "    # Encode the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    y = label_encoder.fit_transform(df[target_column])\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "numerical_features = input(\"Enter numerical features separated by spaces: \").split()\n",
    "text_column = 'tokenized_message_lemmatized_pos'\n",
    "target_column = 'sentiment_label'\n",
    "X, y = feature_extraction(df, text_column, numerical_features, target_column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee97e372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input, Concatenate, LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "def build_train_evaluate_model(df, text_column, numerical_features, target_column, model_type):\n",
    "    if model_type.lower() == 'cnn':\n",
    "        \n",
    "        # CNN Model Building\n",
    "        tokenizer = Tokenizer(num_words=10000)\n",
    "        tokenizer.fit_on_texts(df[text_column])\n",
    "        X_text = tokenizer.texts_to_sequences(df[text_column])\n",
    "        X_text = pad_sequences(X_text, maxlen=100)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_numerical = df[numerical_features].values\n",
    "        X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "        X = np.concatenate((X_text, X_numerical_scaled), axis=1)\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(df[target_column])\n",
    "\n",
    "        input_text = Input(shape=(100,))\n",
    "        embedding = Embedding(input_dim=10000, output_dim=50, input_length=100)(input_text)\n",
    "        conv1d = Conv1D(filters=128, kernel_size=5, activation='relu')(embedding)\n",
    "        global_max_pooling = GlobalMaxPooling1D()(conv1d)\n",
    "\n",
    "        input_numerical = Input(shape=(len(numerical_features),))\n",
    "        concatenated = Concatenate()([global_max_pooling, input_numerical])\n",
    "        dense1 = Dense(64, activation='relu')(concatenated)\n",
    "        dropout = Dropout(0.5)(dense1)\n",
    "        output = Dense(3, activation='softmax')(dropout)\n",
    "\n",
    "        model = Model(inputs=[input_text, input_numerical], outputs=output)\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "        \n",
    "    elif model_type.lower() == 'rnn':\n",
    "        # RNN Model Building\n",
    "        tokenizer = Tokenizer(num_words=10000)\n",
    "        tokenizer.fit_on_texts(df[text_column])\n",
    "        X_text = tokenizer.texts_to_sequences(df[text_column])\n",
    "        X_text = pad_sequences(X_text, maxlen=100)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_numerical = df[numerical_features].values\n",
    "        X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "        X = np.concatenate((X_text, X_numerical_scaled), axis=1)\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(df[target_column])\n",
    "\n",
    "        input_text = Input(shape=(100,))\n",
    "        embedding = Embedding(input_dim=10000, output_dim=50, input_length=100)(input_text)\n",
    "        lstm = LSTM(64)(embedding)\n",
    "        dense1 = Dense(64, activation='relu')(lstm)\n",
    "        dropout = Dropout(0.5)(dense1)\n",
    "        output = Dense(3, activation='softmax')(dropout)\n",
    "\n",
    "        model = Model(inputs=input_text, outputs=output)\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()\n",
    "\n",
    "    elif model_type.lower() == 'kmeans':\n",
    "        # K-Means Model\n",
    "        # Extract features for K-Means\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(df['sentiment_label'])\n",
    "\n",
    "        # Define X based on the features you want to use for clustering\n",
    "        X = df[numerical_features].values  # Adjust this based on your numerical features\n",
    "\n",
    "        # K-Means clustering\n",
    "        num_clusters = int(input(\"Enter the number of clusters: \"))\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "        y_pred = kmeans.fit_predict(X)\n",
    "\n",
    "        # Silhouette Score\n",
    "        silhouette_avg = silhouette_score(X, y_pred)\n",
    "        print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "        # Dimensionality reduction for visualization (e.g., PCA)\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=3)\n",
    "        X_pca = pca.fit_transform(X)\n",
    "\n",
    "        # Visualization of Clusters in 3D after PCA\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        colors = ['r', 'g', 'b']\n",
    "        for i in range(num_clusters):\n",
    "            ax.scatter(X_pca[y_pred == i, 0], X_pca[y_pred == i, 1], X_pca[y_pred == i, 2], c=colors[i], s=50, label=f'Cluster {i}')\n",
    "\n",
    "        ax.set_xlabel('Principal Component 1')\n",
    "        ax.set_ylabel('Principal Component 2')\n",
    "        ax.set_zlabel('Principal Component 3')\n",
    "        ax.set_title('K-Means Clustering based on sentiment_label in 3D')\n",
    "        ax.legend()\n",
    "        plt.show()\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    elif model_type.lower() == 'knn':\n",
    "        # KNN Model\n",
    "        scaler = StandardScaler()\n",
    "        X_numerical = df[numerical_features].values\n",
    "        X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "        knn = KNeighborsClassifier(n_neighbors=3)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_numerical_scaled, df[target_column], test_size=0.2, random_state=42)\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = knn.predict(X_test)\n",
    "\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "\n",
    "        print(\"Accuracy (KNN):\", accuracy)\n",
    "        print(\"Classification Report:\")\n",
    "        print(report)\n",
    "\n",
    "        return\n",
    "\n",
    "    else:\n",
    "        # Machine Learning Models\n",
    "        tokenizer = Tokenizer(num_words=10000)\n",
    "        tokenizer.fit_on_texts(df[text_column])\n",
    "        X_text = tokenizer.texts_to_sequences(df[text_column])\n",
    "        X_text = pad_sequences(X_text, maxlen=100)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_numerical = df[numerical_features].values\n",
    "        X_numerical_scaled = scaler.fit_transform(X_numerical)\n",
    "\n",
    "        X = np.concatenate((X_text, X_numerical_scaled), axis=1)\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(df[target_column])\n",
    "\n",
    "        if model_type.lower() == 'svm':\n",
    "            model = SVC()\n",
    "        elif model_type.lower() == 'logistic':\n",
    "            model = LogisticRegression()\n",
    "        elif model_type.lower() == 'decisiontree':\n",
    "            model = DecisionTreeClassifier()\n",
    "        elif model_type.lower() == 'randomforest':\n",
    "            model = RandomForestClassifier()\n",
    "        elif model_type.lower() == 'xgboost':\n",
    "            model = XGBClassifier()\n",
    "        elif model_type.lower() == 'naivebayes':\n",
    "            model = GaussianNB()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model type. Please choose from: CNN, RNN, SVM, Logistic, DecisionTree, RandomForest, XGBoost, NaiveBayes, KMeans, or KNN.\")\n",
    "\n",
    "    # Model Training\n",
    "    if model_type.lower() == 'cnn':\n",
    "        X_train_text, X_test_text, X_train_numerical, X_test_numerical, y_train, y_test = train_test_split(X_text, X_numerical_scaled, y, test_size=0.2, random_state=42)\n",
    "        model.fit([X_train_text, X_train_numerical], y_train, epochs=10, batch_size=64, validation_data=([X_test_text, X_test_numerical], y_test))\n",
    "        y_pred = np.argmax(model.predict([X_test_text, X_test_numerical]), axis=-1)\n",
    "\n",
    "    elif model_type.lower() == 'rnn':\n",
    "        X_train_text, X_test_text, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42)\n",
    "        model.fit(X_train_text, y_train, epochs=10, batch_size=64, validation_data=(X_test_text, y_test))\n",
    "        y_pred = np.argmax(model.predict(X_test_text), axis=-1)\n",
    "\n",
    "    else:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    # Model Evaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    \n",
    "while True:\n",
    "    # Specify the model type you want to use\n",
    "    model_type = input(\"Enter model type (CNN, RNN, SVM, Logistic, DecisionTree, RandomForest, XGBoost, NaiveBayes, KMeans, KNN): \")\n",
    "\n",
    "    build_train_evaluate_model(df, text_column, numerical_features, target_column, model_type)\n",
    "\n",
    "    choice = input(\"Do you want to try another model? (yes/no): \")\n",
    "    if choice.lower() != 'yes':\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e724f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
